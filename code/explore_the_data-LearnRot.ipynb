{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final competition of Deep Learning 2020 Spring\n",
    "Traffic environment semi-supervised Learning Contest\n",
    "\n",
    "## Goals\n",
    "The objective is to train a model using images captured by six different cameras attached to the same car to generate a top down view of the surrounding area. The performance of the model will be evaluated by (1) the ability of detecting objects (like car, trucks, bicycles, etc.) and (2) the ability to draw the road map layout.\n",
    "\n",
    "## Data\n",
    "You will be given two sets of data:\n",
    "\n",
    " 1. Unlabeled set: just images\n",
    " 2. Labeled set: images and the labels(bounding box and road map layout)\n",
    "\n",
    "This notebook will help you understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "\n",
    "##### ON PRINCE\n",
    "image_folder = '../../DLSP20Dataset/data'\n",
    "annotation_csv = '../../DLSP20Dataset/data/annotation.csv'\n",
    "\n",
    "\n",
    "##### ON WORK LAPTOP\n",
    "#image_folder = '/Users/rasy7001/Documents/DeepLearning/competition /data'\n",
    "#annotation_csv = '/Users/rasy7001/Documents/DeepLearning/competition /data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "def order_points(pts):\n",
    "    from scipy.spatial import distance as dist\n",
    "    import numpy as np\n",
    "    \n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "\n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "\n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\n",
    "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "def arrange_box(x1,y1):\n",
    "    box=np.array(list(zip(x1,y1)))\n",
    "    box=order_points(box)\n",
    "    return box\n",
    "\n",
    "def iou(box1, box2):\n",
    "    from shapely.geometry import Polygon\n",
    "    a = Polygon(torch.t(box1)).convex_hull\n",
    "    b = Polygon(torch.t(box2)).convex_hull\n",
    "    \n",
    "    return a.intersection(b).area / a.union(b).area\n",
    "\n",
    "#def iou(xy1,xy2):\n",
    "#    \n",
    "#    from shapely.geometry import Polygon\n",
    "#    \n",
    "#    boxA = Polygon(arrange_box(xy1[0],xy1[1])).buffer(1e-9)\n",
    "#    boxB = Polygon(arrange_box(xy2[0],xy2[1])).buffer(1e-9)\n",
    "#    \n",
    "#    try:\n",
    "#        return boxA.intersection(boxB).area / boxA.union(boxB).area\n",
    "#    except:\n",
    "#        print('Box 1:',xy1[0],xy1[1])\n",
    "#        print('Box 2:',xy2[0],xy2[1])\n",
    "#        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def calculate_overlap(target_bb, predicted_bb):\n",
    "    overlaps = torch.zeros(target_bb.size(0),predicted_bb.size(0))\n",
    "\n",
    "    for j in range(overlaps.shape[0]):\n",
    "        for k in range(overlaps.shape[1]):\n",
    "            overlaps[j][k] = iou(target_bb[j],predicted_bb[k])\n",
    "            \n",
    "    return overlaps\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import draw\n",
    "import numpy as np\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = torch.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def convert_to_binary_mask(corners, shape=(800,800)):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    x,y = point_squence.T[0].detach() * 10 + 400, -point_squence.T[1].detach() * 10 + 400\n",
    "    new_im = poly2mask(y, x, shape)\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_matrix(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1)\n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    if debug:\n",
    "        print('Target values:', target.unique())\n",
    "        print('Predicted values:', pred.unique())\n",
    "        print('Target shape:', target.shape)\n",
    "        print('Predicted shape:', pred.shape)\n",
    "    \n",
    "    nb_classes = max(target.unique())\n",
    "    if len(pred.unique()) > (nb_classes+1) :\n",
    "        print('More predicted classes than true classes')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    conf_matrix = torch.zeros(nb_classes+1, nb_classes+1)\n",
    "    for t, p in zip(target, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_matrix2(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1).cpu().numpy()\n",
    "    pred = pred.reshape(-1).cpu().numpy()\n",
    "    \n",
    "        \n",
    "    conf_matrix = torch.from_numpy(confusion_matrix(target, pred)).to(device)\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classScores(conf_matrix):\n",
    "    print('Confusion matrix\\n', conf_matrix)\n",
    "    TP = conf_matrix.diag()\n",
    "    TN = torch.zeros_like(TP)\n",
    "    FP = torch.zeros_like(TP)\n",
    "    FN = torch.zeros_like(TP)\n",
    "    for c in range(conf_matrix.size(0)):\n",
    "        idx = torch.ones(conf_matrix.size(0)).byte()\n",
    "        idx[c] = 0\n",
    "        # all non-class samples classified as non-class\n",
    "        TN[c] = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "        # all non-class samples classified as class\n",
    "        FP[c] = conf_matrix[idx, c].sum()\n",
    "        # all class samples not classified as class\n",
    "        FN[c] = conf_matrix[c, idx].sum()\n",
    "\n",
    "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "            c, TP[c], TN[c], FP[c], FN[c]))\n",
    "        \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "You will get two different datasets:\n",
    "\n",
    " 1. an unlabeled dataset for pre-training\n",
    " 2. a labeled dataset for both training and validation\n",
    " \n",
    "## The dataset is organized into three levels: scene, sample and image\n",
    "\n",
    " 1. A scene is 25 seconds of a car's journey.\n",
    " 2. A sample is a snapshot of a scene at a given timeframe. Each scene will be divided into 126 samples, so about 0.2 seconds between consecutive samples.\n",
    " 3. Each sample contains 6 images captured by camera facing different orientation.\n",
    "    Each camera will capture 70 degree view. To make it simple, you can safely assume that the angle between the cameras is 60 degrees \n",
    "\n",
    "106 scenes in the unlabeled dataset and 28 scenes in the labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for rotation \n",
    "train_scene_index = np.random.choice(unlabeled_scene_index, int(np.ceil(0.95*len(unlabeled_scene_index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for rotation \n",
    "test_scene_index = unlabeled_scene_index[np.isin(unlabeled_scene_index, train_scene_index, invert=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled dataset\n",
    "\n",
    "You get two ways to access the dataset, by sample or by image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "transform=torchvision.transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                          torchvision.transforms.ToTensor(),\n",
    "                              torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "\n",
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, \n",
    "                                      scene_index=train_scene_index, \n",
    "                                      first_dim='sample', \n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "unlabeled_testset = UnlabeledDataset(image_folder=image_folder, \n",
    "                                      scene_index=test_scene_index, \n",
    "                                      first_dim='sample', \n",
    "                                      transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(unlabeled_testset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The 6 images orgenized in the following order:\n",
    "# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT\n",
    "plt.imshow(torchvision.utils.make_grid(sample[2], nrow=3).numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 bias = True, \n",
    "                 pool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        if pool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                nn.MaxPool2d(kernel_size=mp_kernel_size, stride=mp_stride))\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            torch.nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(negative_slope=0.1) ## nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ConvTLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 output_padding=0, \n",
    "                 unpool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvTLayer, self).__init__()\n",
    "        if unpool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "                nn.MaxUnpool2d(kernel_size=mp_kernel_size, stride=mp_stride)\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "            )        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.conv1 = ConvLayer(3,96, stride=2)\n",
    "        self.conv2 = ConvLayer(96,128, stride=2)\n",
    "        self.conv3 = ConvLayer(128,256, stride=2)\n",
    "        self.conv4 = ConvLayer(256,512, stride=2)\n",
    "        self.conv5 = ConvLayer(512,1024, stride=2)\n",
    "        self.conv6 = ConvLayer(1024,2048, stride=2)\n",
    "        self.lin1 = nn.Linear(2048*3*3, d)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.lin1(x.reshape(-1,2048*3*3))\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, d=650, output_size=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.encoder = Encoder1(d=d)\n",
    "        self.linear = nn.Linear(d,4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset class for unlabeled data.\n",
    "class UnlabeledRotationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, scene_index, first_dim, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): the location of the image folder\n",
    "            scene_index (list): a list of scene indices for the unlabeled data \n",
    "            first_dim ({'sample', 'image'}):\n",
    "                'sample' will return [batch_size, NUM_IMAGE_PER_SAMPLE, 3, H, W]\n",
    "                'image' will return [batch_size, 3, H, W] and the index of the camera [0 - 5]\n",
    "                    CAM_FRONT_LEFT: 0\n",
    "                    CAM_FRONT: 1\n",
    "                    CAM_FRONT_RIGHT: 2\n",
    "                    CAM_BACK_LEFT: 3\n",
    "                    CAM_BACK.jpeg: 4\n",
    "                    CAM_BACK_RIGHT: 5\n",
    "            transform (Transform): The function to process the image\n",
    "        \"\"\"\n",
    "\n",
    "        self.image_folder = image_folder\n",
    "        self.scene_index = scene_index\n",
    "        self.transform = transform\n",
    "\n",
    "        self.first_dim = 'image'\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.first_dim == 'sample':\n",
    "            return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "        elif self.first_dim == 'image':\n",
    "            return self.scene_index.size * NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.first_dim == 'sample':\n",
    "            scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "            sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "            sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}') \n",
    "\n",
    "            images = []\n",
    "            labels = []\n",
    "            for image_name in image_names:\n",
    "                \n",
    "                rot_class = np.random.randint(4)\n",
    "                rot_angle = rot_class * 90\n",
    "                \n",
    "                image_path = os.path.join(sample_path, image_name)\n",
    "                image = Image.open(image_path)\n",
    "                rot_image = image.rotate(rot_angle)\n",
    "                labels.append(torch.from_numpy(np.asarray(rot_class)))\n",
    "                images.append(self.transform(image))\n",
    "            \n",
    "            image_tensor = torch.stack(images)\n",
    "            label_tensor = torch.stack(labels)\n",
    "            \n",
    "            return image_tensor, label_tensor\n",
    "            \n",
    "        elif self.first_dim == 'image':\n",
    "            scene_id = self.scene_index[index // (NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE)]\n",
    "            sample_id = (index % (NUM_SAMPLE_PER_SCENE * NUM_IMAGE_PER_SAMPLE)) // NUM_IMAGE_PER_SAMPLE\n",
    "            image_name = image_names[index % NUM_IMAGE_PER_SAMPLE]\n",
    "\n",
    "            image_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}', image_name) \n",
    "\n",
    "            rot_class = np.random.randint(4)\n",
    "            rot_angle = rot_class * 90\n",
    "            image = Image.open(image_path)\n",
    "            rot_image = image.rotate(rot_angle)\n",
    "\n",
    "            return self.transform(rot_image), torch.from_numpy(np.asarray(rot_class)), index % NUM_IMAGE_PER_SAMPLE\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get individual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, \n",
    "                                      scene_index=unlabeled_scene_index, \n",
    "                                      first_dim='image', \n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=2, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, 3, H, W]\n",
    "image, camera_index = iter(trainloader).next()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# Camera_index is to tell you which camera is used. The order is\n",
    "# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT\n",
    "print(camera_index[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(image[0].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_trainset = UnlabeledRotationDataset(image_folder=image_folder, \n",
    "                                      scene_index=train_scene_index, \n",
    "                                      first_dim='image', \n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "unlabeled_testset = UnlabeledRotationDataset(image_folder=image_folder, \n",
    "                                      scene_index=test_scene_index, \n",
    "                                      first_dim='image', \n",
    "                                      transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(unlabeled_testset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, label, img_index = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 37176950\n",
      "Train Epoch: 0 [0/76356 (0%)]\tLoss: 1.493132\n",
      "Train Epoch: 0 [16000/76356 (21%)]\tLoss: 0.003617\n",
      "Train Epoch: 0 [32000/76356 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 0 [48000/76356 (63%)]\tLoss: 1.340261\n",
      "Train Epoch: 0 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.2612, Accuracy: 30472/30996 (98%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 1 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/76356 (21%)]\tLoss: 0.043526\n",
      "Train Epoch: 1 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.8406, Accuracy: 30359/30996 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/76356 (0%)]\tLoss: 2.571998\n",
      "Train Epoch: 2 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.3921, Accuracy: 30743/30996 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.2506, Accuracy: 30812/30996 (99%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 4 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.2661, Accuracy: 30789/30996 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0870, Accuracy: 30944/30996 (100%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 6 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.1618, Accuracy: 30922/30996 (100%)\n",
      "\n",
      "Train Epoch: 7 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.1150, Accuracy: 30945/30996 (100%)\n",
      "\n",
      "Train Epoch: 8 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0859, Accuracy: 30959/30996 (100%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 9 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.2573, Accuracy: 30928/30996 (100%)\n",
      "\n",
      "Train Epoch: 10 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.1150, Accuracy: 30955/30996 (100%)\n",
      "\n",
      "Train Epoch: 11 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.2699, Accuracy: 30934/30996 (100%)\n",
      "\n",
      "Train Epoch: 12 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.5982, Accuracy: 30855/30996 (100%)\n",
      "\n",
      "Train Epoch: 13 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.4310, Accuracy: 30891/30996 (100%)\n",
      "\n",
      "Train Epoch: 14 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.1871, Accuracy: 30933/30996 (100%)\n",
      "\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Train Epoch: 15 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.1005, Accuracy: 30964/30996 (100%)\n",
      "\n",
      "Train Epoch: 16 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0584, Accuracy: 30973/30996 (100%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 17 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 30980/30996 (100%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 18 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 18 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 18 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 18 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 18 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 30984/30996 (100%)\n",
      "\n",
      "Updating best model\n",
      "Train Epoch: 19 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 19 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 30979/30996 (100%)\n",
      "\n",
      "Train Epoch: 20 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 20 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 20 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 20 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 20 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0604, Accuracy: 30980/30996 (100%)\n",
      "\n",
      "Train Epoch: 21 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 21 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 21 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 21 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 21 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 30978/30996 (100%)\n",
      "\n",
      "Train Epoch: 22 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 22 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 22 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 22 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 22 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0503, Accuracy: 30976/30996 (100%)\n",
      "\n",
      "Train Epoch: 23 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 23 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 23 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 23 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 23 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 30980/30996 (100%)\n",
      "\n",
      "Train Epoch: 24 [0/76356 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 24 [16000/76356 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 24 [32000/76356 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 24 [48000/76356 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 24 [64000/76356 (84%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0553, Accuracy: 30979/30996 (100%)\n",
      "\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "best_loss = 1000000\n",
    "\n",
    "model_cnn = CNN()\n",
    "model_cnn.to(device)\n",
    "best_model = copy.deepcopy(model_cnn)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_cnn.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       mode='min', \n",
    "                                                       factor=0.1, \n",
    "                                                       patience=5,\n",
    "                                                       verbose=True)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, data_idx) in enumerate(trainloader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "            \n",
    "def test(model):\n",
    "    global best_loss, best_model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target, data_idx in testloader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100. * correct / len(testloader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "        accuracy))\n",
    "    if test_loss < best_loss:\n",
    "        print('Updating best model')\n",
    "        best_loss = copy.deepcopy(test_loss)\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'models/rotation_learning_model.pth')\n",
    "        \n",
    "    return test_loss\n",
    "        \n",
    "for epoch in range(0, 25):\n",
    "    train(epoch, model_cnn)\n",
    "    test_loss = test(model_cnn)\n",
    "    scheduler.step(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
